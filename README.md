# NLP-Pipeline
This repository contains the files/programs used as part of a NLP pipeline consisting of data cleaning and model training/prediction. 

## Motivation

The pipeline created and analysis/model development conducted in this repository was part of the second project in the Udacity Data Science Nanodegree.

The objective of this pipeline is to:
1. Read in text in the form of private messages, public social media posts and news related to disaster events
2. Classify these messages based on 36 pre-defined categories, e.g. 'water', 'earthquake', 'search_and_rescue'. Messages can fall into multiple categories

In the event of a disaster, such a pipeline could be used to help connect relevant agencies or organizations to those in need, reducing the time taken to respond to potentially critical situations.

## Installations
The Anaconda distribution is required to run the code in this repository.

The necessary libraries to run the .py scripts are: 
- pandas
- numpy
- nltk
- scikit-learn

The version of python used for running the .py scripts is Python 3.6.3. 

In addition to the above libraries, the following libraries are needed to run the .ipynb file:
-xgboost
-hyperopt

The version of python used for the analysis/model development is Python 3.10.9. 

## Files in this repository
The files are organized according to the following schematic:
- Root Directory
    - data
        - processing_data.py
        - disaster_categories.csv
        - disaster_messages.csv
    - models
        - train_classifier.py
    - app
        - run.py
    - model_tuning.ipynb

### disaster_messages.csv & disaster_categories.csv

disaster_messages.csv contains messages relevant to disasters, while disaster_categories.csv contains data classifying how the messages are relevant.

### process_data.py

This script takes in the files data/disaster_messages.csv and data/disaster_categories.csv, merges and cleans the data, then outputs the processed data to a .db file.

To run this script from the root directory: python data/process_data.py data/disaster_messages.csv data/disaster_categories.csv data/DisasterResponse.db

### train_classifier.py

This script reads in the .db file generated by process_data.py, trains the model, then outputs the model to a .pkl file.

To run this script from the root directory: python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl

### run.py

This script creates a local web app showcasing the results of the pipeline.

To run this script, navigate to the 'app' directory and run: python run.py

### model_tuning.ipynb

Due to the poor performance of the random forest classifier, I explored the xgboost classifier as a possible alternative. While the xgboost classifier performed much better than the random forest classifier, I was unfortunately could not implement it sucessfully, as training on the web IDE provided by Udacity for this project took far too long (> 1 hour). 
